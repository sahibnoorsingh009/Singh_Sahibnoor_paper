# -*- coding: utf-8 -*-
"""party_classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uxCxTnm_4sHDuOC3YcKaq1IJLZyNZ8xb
"""

import pandas as pd
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
import joblib

# Define a text cleaning function
stop_words = set(stopwords.words('english'))

def clean_text(text):
    text = text.lower()
    text = re.sub(r'http\S+', '', text)     # remove URLs
    text = re.sub(r'@\w+', '', text)          # remove mentions
    text = re.sub(r'#\w+', '', text)          # remove hashtags
    text = re.sub(r'[^a-z\s]', '', text)      # remove punctuation and numbers
    text = ' '.join([word for word in text.split() if word not in stop_words])
    return text

# Load your dataset (adjust the file path or DataFrame as needed)
df = pd.read_csv('/content/drive/MyDrive/output.csv')  # replace with your actual file path if different

# Clean the tweet text
df['clean_text'] = df['tweet_text'].apply(clean_text)

# Vectorize the cleaned text using TF-IDF
tfidf = TfidfVectorizer()
X = tfidf.fit_transform(df['clean_text'])

# Encode the party labels
le = LabelEncoder()
y = le.fit_transform(df['party'])

# Fit a Random Forest Classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X, y)

# Save the trained model, vectorizer, and label encoder
joblib.dump(tfidf, 'tfidf_vectorizer.pkl')
joblib.dump(le, 'label_encoder.pkl')
joblib.dump(clf, 'random_forest_model.pkl')

print("Model training complete and components saved.")

import pandas as pd
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
import joblib

# Define the same cleaning function used during training
stop_words = set(stopwords.words('english'))

def clean_text(text):
    text = text.lower()
    text = re.sub(r'http\S+', '', text)     # remove URLs
    text = re.sub(r'@\w+', '', text)          # remove mentions
    text = re.sub(r'#\w+', '', text)          # remove hashtags
    text = re.sub(r'[^a-z\s]', '', text)      # remove punctuation and numbers
    text = ' '.join([word for word in text.split() if word not in stop_words])
    return text

# Load your new dataset (assumes the column with text is named 'text')
new_df = pd.read_csv('/content/drive/MyDrive/sorted_output.csv')  # update with your file name/path

# Clean the input text column
new_df['clean_text'] = new_df['text'].apply(clean_text)

# Load the saved TF-IDF vectorizer, label encoder, and Random Forest model
tfidf = joblib.load('tfidf_vectorizer.pkl')
le = joblib.load('label_encoder.pkl')
clf = joblib.load('random_forest_model.pkl')

# Transform the cleaned text into TF-IDF features
X_new = tfidf.transform(new_df['clean_text'])

# Predict the encoded party labels
predictions = clf.predict(X_new)

# Inverse transform the numerical predictions back to party names
predicted_parties = le.inverse_transform(predictions)

# Add the predicted party labels as a new column in the dataframe
new_df['party'] = predicted_parties

# Optionally, save the updated dataframe with predictions to a new CSV file
new_df.to_csv('new_dataset_with_predictions.csv', index=False)

print("Predictions added and new dataset saved as 'new_dataset_with_predictions.csv'.")

import pandas as pd

# Load your dataset (replace 'your_file.csv' with your actual file)
df = pd.read_csv('/content/drive/MyDrive/new_dataset_with_predictions.csv')

# Keep only the required columns
columns_to_keep = ['text', 'clean_text', 'in_reply_to_screen_name', 'date', 'party']
df = df[columns_to_keep]

# Save the cleaned dataset if needed
df.to_csv('election2024_data.csv', index=False)

# Display the first few rows
print(df.head())